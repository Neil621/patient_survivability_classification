{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78ccf1f3-4fbd-471f-9d6f-09f92527f9e3",
   "metadata": {},
   "source": [
    "# Final Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763102a4-03dc-436e-9ec1-2e7676738834",
   "metadata": {},
   "source": [
    "## 1. Dataset Selection and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6064f572-3ebd-47a6-bce5-9562810ad9c4",
   "metadata": {},
   "source": [
    "### 1.1 Dataset Selection and Preparation: Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6412c-81c9-4c41-aee3-d26905ea38b4",
   "metadata": {},
   "source": [
    "I chose the Support dataset provided by Vanderbilt University Department of Biostatistics and hosted on the UC Irving Machine Learning Repository (https://archive.ics.uci.edu/dataset/880/support2, https://hbiostat.org/data/repo/supportdesc). The Robert Wood Johnson Foundation funded the creation of this dataset (https://www.rwjf.org/).\n",
    "\n",
    "The Support dataset is comprised of 9105 individual critically ill patients accross 5 medical centers in the United States over 1989-1991 and 1992-1994. The original goal of the dataset was to determine the patients 2 and 6 month survival rates based on available information including physiological, demographical and disease severity or symptom information. The Support dataset has 9105 instances and 42 features with various continous and categorical variables. This is a very rich dataset that hopefully will be of interest to a wide range of audiences.\n",
    "\n",
    "I aim to explore the use of neural networks and logistic regression models to predict whether or not a patient will die \"hospdead\" which is a binary 0, 1 variable. Given it is a classification task I have disregarded the use of a linear regression and consider instead logistic regression and neural network models. \n",
    "\n",
    "It is recognised that some of the information in relation to particularly gender, race, income and educational level should be approached in a sensitive way but nonetheless is deemed potentially useful and insightful in understanding outcomes of critically ill patients. Data provided has been anonymised and individual patients are not identifiable in any way. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "d33edfc3-1907-4d10-9ee9-d43ce58746b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Modelling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Standard imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Required for modeling and evaluation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#nnet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "d793a270-d9df-4618-b5fe-52238f8c079a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " # fetch dataset \n",
    "support2 = fetch_ucirepo(id=880) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = support2.data.features \n",
    "y = support2.data.targets \n",
    "\n",
    "  \n",
    "# variable information \n",
    "#I use this reference information later when I describe and relate the features of X\n",
    "#print(support2.variables) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa302b3-b31c-4eb9-893f-844d727cdcb1",
   "metadata": {},
   "source": [
    "### 1.2 Dataset Selection and Preparation: Overview of processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a9cdf5-ee77-44d0-a2b0-be539366e7b4",
   "metadata": {},
   "source": [
    "The process used in this projet was as follows:\n",
    "* Data preprocessing and cleaning stage\n",
    "* Splitting into train and test sets\n",
    "* Preliminary Model 1 Logistic Regression\n",
    "* Data Exploration and curation of training features\n",
    "* Training of Models 2-4 Logistic Regressions on various curated data\n",
    "* Decision Tree Model 5\n",
    "* Neural Network Model 6\n",
    "* Model Evaluation and discussion of models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13884316-fdcc-4315-8055-485238444778",
   "metadata": {},
   "source": [
    "### 1.1 Dataset Selection and Preparation. Summary of dependant and independant variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ef05e9-26c1-4c03-b34d-27f5f4f27c0c",
   "metadata": {},
   "source": [
    "In order to understand better the patient dataset both to validate it and also to allow for more meainginful evaluation and interpretation of the later modelling steps I applied the described function to the dependant variable and independant variables.\n",
    "\n",
    "There is nothing particularly noteworthy in terms of the X variables the explanatory or independant variables except that we can see that whereas we had 42 columns originally we now have only 35 since some of the cateogrical variables have not been encoded accordingly. This will mean that I will need to encode the missing items separately in the processing stage.\n",
    "\n",
    "\n",
    "However for Y below it is worth noting that 25.9% of patients did die in hospital. This is noteworthy as it means that there is a class imbalance with more patients surviving than not; this means that we will want to make use of micro averaging to correct for the class imbalance later on when we move into the Model Evaluation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "839ea400-d670-4246-a677-28a502ac657d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>num.co</th>\n",
       "      <th>edu</th>\n",
       "      <th>scoma</th>\n",
       "      <th>charges</th>\n",
       "      <th>totcst</th>\n",
       "      <th>totmcst</th>\n",
       "      <th>avtisst</th>\n",
       "      <th>sps</th>\n",
       "      <th>aps</th>\n",
       "      <th>...</th>\n",
       "      <th>bili</th>\n",
       "      <th>crea</th>\n",
       "      <th>sod</th>\n",
       "      <th>ph</th>\n",
       "      <th>glucose</th>\n",
       "      <th>bun</th>\n",
       "      <th>urine</th>\n",
       "      <th>adlp</th>\n",
       "      <th>adls</th>\n",
       "      <th>adlsc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9105.000000</td>\n",
       "      <td>9105.000000</td>\n",
       "      <td>7471.000000</td>\n",
       "      <td>9104.000000</td>\n",
       "      <td>8.933000e+03</td>\n",
       "      <td>8217.000000</td>\n",
       "      <td>5630.000000</td>\n",
       "      <td>9023.000000</td>\n",
       "      <td>9104.000000</td>\n",
       "      <td>9104.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6504.000000</td>\n",
       "      <td>9038.000000</td>\n",
       "      <td>9104.000000</td>\n",
       "      <td>6821.000000</td>\n",
       "      <td>4605.000000</td>\n",
       "      <td>4753.000000</td>\n",
       "      <td>4243.000000</td>\n",
       "      <td>3464.000000</td>\n",
       "      <td>6238.000000</td>\n",
       "      <td>9105.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>62.650823</td>\n",
       "      <td>1.868644</td>\n",
       "      <td>11.747691</td>\n",
       "      <td>12.058546</td>\n",
       "      <td>5.999579e+04</td>\n",
       "      <td>30825.867768</td>\n",
       "      <td>28828.877838</td>\n",
       "      <td>22.610928</td>\n",
       "      <td>25.525872</td>\n",
       "      <td>37.597979</td>\n",
       "      <td>...</td>\n",
       "      <td>2.554463</td>\n",
       "      <td>1.770961</td>\n",
       "      <td>137.568541</td>\n",
       "      <td>7.415364</td>\n",
       "      <td>159.873398</td>\n",
       "      <td>32.349463</td>\n",
       "      <td>2191.546047</td>\n",
       "      <td>1.157910</td>\n",
       "      <td>1.637384</td>\n",
       "      <td>1.888272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.593710</td>\n",
       "      <td>1.344409</td>\n",
       "      <td>3.447743</td>\n",
       "      <td>24.636694</td>\n",
       "      <td>1.026488e+05</td>\n",
       "      <td>45780.820986</td>\n",
       "      <td>43604.261932</td>\n",
       "      <td>13.233248</td>\n",
       "      <td>9.899377</td>\n",
       "      <td>19.903852</td>\n",
       "      <td>...</td>\n",
       "      <td>5.318448</td>\n",
       "      <td>1.686041</td>\n",
       "      <td>6.029326</td>\n",
       "      <td>0.080563</td>\n",
       "      <td>88.391541</td>\n",
       "      <td>26.792288</td>\n",
       "      <td>1455.245777</td>\n",
       "      <td>1.739672</td>\n",
       "      <td>2.231358</td>\n",
       "      <td>2.003763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.041990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.169000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-102.719970</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.199982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099991</td>\n",
       "      <td>0.099991</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>6.829102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>52.797000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.740000e+03</td>\n",
       "      <td>5929.566400</td>\n",
       "      <td>5177.404300</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.899902</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>7.379883</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1165.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>64.856990</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.502400e+04</td>\n",
       "      <td>14452.734400</td>\n",
       "      <td>13223.500000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>23.898438</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.899902</td>\n",
       "      <td>1.199951</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>7.419922</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1968.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>73.998960</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.459800e+04</td>\n",
       "      <td>36087.937500</td>\n",
       "      <td>34223.601600</td>\n",
       "      <td>31.666656</td>\n",
       "      <td>30.199219</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.899902</td>\n",
       "      <td>1.899902</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>7.469727</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>101.847960</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.435423e+06</td>\n",
       "      <td>633212.000000</td>\n",
       "      <td>710682.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>99.187500</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>7.769531</td>\n",
       "      <td>1092.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>9000.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.073242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               age       num.co          edu        scoma       charges  \\\n",
       "count  9105.000000  9105.000000  7471.000000  9104.000000  8.933000e+03   \n",
       "mean     62.650823     1.868644    11.747691    12.058546  5.999579e+04   \n",
       "std      15.593710     1.344409     3.447743    24.636694  1.026488e+05   \n",
       "min      18.041990     0.000000     0.000000     0.000000  1.169000e+03   \n",
       "25%      52.797000     1.000000    10.000000     0.000000  9.740000e+03   \n",
       "50%      64.856990     2.000000    12.000000     0.000000  2.502400e+04   \n",
       "75%      73.998960     3.000000    14.000000     9.000000  6.459800e+04   \n",
       "max     101.847960     9.000000    31.000000   100.000000  1.435423e+06   \n",
       "\n",
       "              totcst        totmcst      avtisst          sps          aps  \\\n",
       "count    8217.000000    5630.000000  9023.000000  9104.000000  9104.000000   \n",
       "mean    30825.867768   28828.877838    22.610928    25.525872    37.597979   \n",
       "std     45780.820986   43604.261932    13.233248     9.899377    19.903852   \n",
       "min         0.000000    -102.719970     1.000000     0.199982     0.000000   \n",
       "25%      5929.566400    5177.404300    12.000000    19.000000    23.000000   \n",
       "50%     14452.734400   13223.500000    19.500000    23.898438    34.000000   \n",
       "75%     36087.937500   34223.601600    31.666656    30.199219    49.000000   \n",
       "max    633212.000000  710682.000000    83.000000    99.187500   143.000000   \n",
       "\n",
       "       ...         bili         crea          sod           ph      glucose  \\\n",
       "count  ...  6504.000000  9038.000000  9104.000000  6821.000000  4605.000000   \n",
       "mean   ...     2.554463     1.770961   137.568541     7.415364   159.873398   \n",
       "std    ...     5.318448     1.686041     6.029326     0.080563    88.391541   \n",
       "min    ...     0.099991     0.099991   110.000000     6.829102     0.000000   \n",
       "25%    ...     0.500000     0.899902   134.000000     7.379883   103.000000   \n",
       "50%    ...     0.899902     1.199951   137.000000     7.419922   135.000000   \n",
       "75%    ...     1.899902     1.899902   141.000000     7.469727   188.000000   \n",
       "max    ...    63.000000    21.500000   181.000000     7.769531  1092.000000   \n",
       "\n",
       "               bun        urine         adlp         adls        adlsc  \n",
       "count  4753.000000  4243.000000  3464.000000  6238.000000  9105.000000  \n",
       "mean     32.349463  2191.546047     1.157910     1.637384     1.888272  \n",
       "std      26.792288  1455.245777     1.739672     2.231358     2.003763  \n",
       "min       1.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%      14.000000  1165.500000     0.000000     0.000000     0.000000  \n",
       "50%      23.000000  1968.000000     0.000000     1.000000     1.000000  \n",
       "75%      42.000000  3000.000000     2.000000     3.000000     3.000000  \n",
       "max     300.000000  9000.000000     7.000000     7.000000     7.073242  \n",
       "\n",
       "[8 rows x 35 columns]"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "44be5cba-6e82-4638-8275-174374d67760",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hospdead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9105.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.259198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.438219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          hospdead\n",
       "count  9105.000000\n",
       "mean      0.259198\n",
       "std       0.438219\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I extracted the \"hospdead\" with 0 for survived and 1 for died in hospital\n",
    "Y=y[['hospdead']]\n",
    "Y\n",
    "\n",
    "Y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2f460-7626-40ef-b9e4-bd3165d8ff3f",
   "metadata": {},
   "source": [
    "## 2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a5ecdb-6bd5-4df6-bf2a-979b7a2216bf",
   "metadata": {},
   "source": [
    "### 2.1 Data Preprocessing: Evaluating and Processing NA values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad94f6b-9867-4ded-8f59-465f20b42aad",
   "metadata": {},
   "source": [
    "I checked both X, the independant or features, and Y the dependant variable (\"hospdead\") for the presence of NAs.\n",
    "\n",
    "I find that Y has no NAs, while X has NAs in some columns and some columns have many NAs. For example we see that totmcst has 3475 NAs, given that we only have 9105 observations this is very significant (38% of entries) while chagres had 172 NAs amounting to 1.9% of all observations. In order to simplify data processing and intepretability I removed any column that had more 10% or more NAs (i.e 911) as at that point I believed it would interfere too much it the final results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "121be17d-47e9-4e64-847f-27bd939f1641",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hospdead    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "3e6d376e-f979-4fc3-8e5b-2bc35ceb7702",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age            0\n",
       "sex            0\n",
       "dzgroup        0\n",
       "dzclass        0\n",
       "num.co         0\n",
       "edu         1634\n",
       "income      2982\n",
       "scoma          1\n",
       "charges      172\n",
       "totcst       888\n",
       "totmcst     3475\n",
       "avtisst       82\n",
       "race          42\n",
       "sps            1\n",
       "aps            1\n",
       "surv2m         1\n",
       "surv6m         1\n",
       "hday           0\n",
       "diabetes       0\n",
       "dementia       0\n",
       "ca             0\n",
       "prg2m       1649\n",
       "prg6m       1633\n",
       "dnr           30\n",
       "dnrday        30\n",
       "meanbp         1\n",
       "wblc         212\n",
       "hrt            1\n",
       "resp           1\n",
       "temp           1\n",
       "pafi        2325\n",
       "alb         3372\n",
       "bili        2601\n",
       "crea          67\n",
       "sod            1\n",
       "ph          2284\n",
       "glucose     4500\n",
       "bun         4352\n",
       "urine       4862\n",
       "adlp        5641\n",
       "adls        2867\n",
       "adlsc          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "X.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "2cb8c220-1a1e-48e9-b15c-3b7ccc36091e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           0\n",
       "sex           0\n",
       "dzgroup       0\n",
       "dzclass       0\n",
       "num.co        0\n",
       "scoma         1\n",
       "charges     172\n",
       "totcst      888\n",
       "avtisst      82\n",
       "race         42\n",
       "sps           1\n",
       "aps           1\n",
       "surv2m        1\n",
       "surv6m        1\n",
       "hday          0\n",
       "diabetes      0\n",
       "dementia      0\n",
       "ca            0\n",
       "dnr          30\n",
       "dnrday       30\n",
       "meanbp        1\n",
       "wblc        212\n",
       "hrt           1\n",
       "resp          1\n",
       "temp          1\n",
       "crea         67\n",
       "sod           1\n",
       "adlsc         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing all columns with more than 10% NANs\n",
    "\n",
    "X2=X.copy()\n",
    "X2=X2.drop(X2.columns[X2.isnull().mean()>0.1], axis=1)\n",
    "X2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8953d267-c071-4a07-a55e-ba2a5dbbe3cb",
   "metadata": {},
   "source": [
    "### 2.4 Data Preprocessing: Imputing NA values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c95630-d8ef-4b4a-b6d6-53867af0de41",
   "metadata": {},
   "source": [
    "In order to help determine how best to deal with the remaining NAs I use describe on the columns that remain that also have NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "5b9228a5-7cda-499a-b199-1cb6a8c15f16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>num.co</th>\n",
       "      <th>scoma</th>\n",
       "      <th>charges</th>\n",
       "      <th>totcst</th>\n",
       "      <th>avtisst</th>\n",
       "      <th>sps</th>\n",
       "      <th>aps</th>\n",
       "      <th>surv2m</th>\n",
       "      <th>surv6m</th>\n",
       "      <th>...</th>\n",
       "      <th>dementia</th>\n",
       "      <th>dnrday</th>\n",
       "      <th>meanbp</th>\n",
       "      <th>wblc</th>\n",
       "      <th>hrt</th>\n",
       "      <th>resp</th>\n",
       "      <th>temp</th>\n",
       "      <th>crea</th>\n",
       "      <th>sod</th>\n",
       "      <th>adlsc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9105.000000</td>\n",
       "      <td>9105.000000</td>\n",
       "      <td>9104.000000</td>\n",
       "      <td>8.933000e+03</td>\n",
       "      <td>8217.000000</td>\n",
       "      <td>9023.000000</td>\n",
       "      <td>9104.000000</td>\n",
       "      <td>9104.000000</td>\n",
       "      <td>9104.000000</td>\n",
       "      <td>9104.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9105.000000</td>\n",
       "      <td>9075.000000</td>\n",
       "      <td>9104.000000</td>\n",
       "      <td>8893.000000</td>\n",
       "      <td>9104.000000</td>\n",
       "      <td>9104.000000</td>\n",
       "      <td>9104.000000</td>\n",
       "      <td>9038.000000</td>\n",
       "      <td>9104.000000</td>\n",
       "      <td>9105.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>62.650823</td>\n",
       "      <td>1.868644</td>\n",
       "      <td>12.058546</td>\n",
       "      <td>5.999579e+04</td>\n",
       "      <td>30825.867768</td>\n",
       "      <td>22.610928</td>\n",
       "      <td>25.525872</td>\n",
       "      <td>37.597979</td>\n",
       "      <td>0.635870</td>\n",
       "      <td>0.520096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032510</td>\n",
       "      <td>14.582590</td>\n",
       "      <td>84.546408</td>\n",
       "      <td>12.347677</td>\n",
       "      <td>97.156711</td>\n",
       "      <td>23.330294</td>\n",
       "      <td>37.103341</td>\n",
       "      <td>1.770961</td>\n",
       "      <td>137.568541</td>\n",
       "      <td>1.888272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>15.593710</td>\n",
       "      <td>1.344409</td>\n",
       "      <td>24.636694</td>\n",
       "      <td>1.026488e+05</td>\n",
       "      <td>45780.820986</td>\n",
       "      <td>13.233248</td>\n",
       "      <td>9.899377</td>\n",
       "      <td>19.903852</td>\n",
       "      <td>0.248175</td>\n",
       "      <td>0.253343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.177359</td>\n",
       "      <td>19.944216</td>\n",
       "      <td>27.687692</td>\n",
       "      <td>9.266329</td>\n",
       "      <td>31.559292</td>\n",
       "      <td>9.573801</td>\n",
       "      <td>1.251796</td>\n",
       "      <td>1.686041</td>\n",
       "      <td>6.029326</td>\n",
       "      <td>2.003763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.041990</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.169000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.199982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-88.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.699220</td>\n",
       "      <td>0.099991</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>52.797000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.740000e+03</td>\n",
       "      <td>5929.566400</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.507690</td>\n",
       "      <td>0.342957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>36.195310</td>\n",
       "      <td>0.899902</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>64.856990</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.502400e+04</td>\n",
       "      <td>14452.734400</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>23.898438</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.715942</td>\n",
       "      <td>0.574951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>10.599609</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>36.695310</td>\n",
       "      <td>1.199951</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>73.998960</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.459800e+04</td>\n",
       "      <td>36087.937500</td>\n",
       "      <td>31.666656</td>\n",
       "      <td>30.199219</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>0.825928</td>\n",
       "      <td>0.725952</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>15.298828</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>38.195310</td>\n",
       "      <td>1.899902</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>101.847960</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.435423e+06</td>\n",
       "      <td>633212.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>99.187500</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>0.969971</td>\n",
       "      <td>0.947998</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>285.000000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>41.695310</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>7.073242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               age       num.co        scoma       charges         totcst  \\\n",
       "count  9105.000000  9105.000000  9104.000000  8.933000e+03    8217.000000   \n",
       "mean     62.650823     1.868644    12.058546  5.999579e+04   30825.867768   \n",
       "std      15.593710     1.344409    24.636694  1.026488e+05   45780.820986   \n",
       "min      18.041990     0.000000     0.000000  1.169000e+03       0.000000   \n",
       "25%      52.797000     1.000000     0.000000  9.740000e+03    5929.566400   \n",
       "50%      64.856990     2.000000     0.000000  2.502400e+04   14452.734400   \n",
       "75%      73.998960     3.000000     9.000000  6.459800e+04   36087.937500   \n",
       "max     101.847960     9.000000   100.000000  1.435423e+06  633212.000000   \n",
       "\n",
       "           avtisst          sps          aps       surv2m       surv6m  ...  \\\n",
       "count  9023.000000  9104.000000  9104.000000  9104.000000  9104.000000  ...   \n",
       "mean     22.610928    25.525872    37.597979     0.635870     0.520096  ...   \n",
       "std      13.233248     9.899377    19.903852     0.248175     0.253343  ...   \n",
       "min       1.000000     0.199982     0.000000     0.000000     0.000000  ...   \n",
       "25%      12.000000    19.000000    23.000000     0.507690     0.342957  ...   \n",
       "50%      19.500000    23.898438    34.000000     0.715942     0.574951  ...   \n",
       "75%      31.666656    30.199219    49.000000     0.825928     0.725952  ...   \n",
       "max      83.000000    99.187500   143.000000     0.969971     0.947998  ...   \n",
       "\n",
       "          dementia       dnrday       meanbp         wblc          hrt  \\\n",
       "count  9105.000000  9075.000000  9104.000000  8893.000000  9104.000000   \n",
       "mean      0.032510    14.582590    84.546408    12.347677    97.156711   \n",
       "std       0.177359    19.944216    27.687692     9.266329    31.559292   \n",
       "min       0.000000   -88.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     4.000000    63.000000     7.000000    72.000000   \n",
       "50%       0.000000     9.000000    77.000000    10.599609   100.000000   \n",
       "75%       0.000000    17.000000   107.000000    15.298828   120.000000   \n",
       "max       1.000000   285.000000   195.000000   200.000000   300.000000   \n",
       "\n",
       "              resp         temp         crea          sod        adlsc  \n",
       "count  9104.000000  9104.000000  9038.000000  9104.000000  9105.000000  \n",
       "mean     23.330294    37.103341     1.770961   137.568541     1.888272  \n",
       "std       9.573801     1.251796     1.686041     6.029326     2.003763  \n",
       "min       0.000000    31.699220     0.099991   110.000000     0.000000  \n",
       "25%      18.000000    36.195310     0.899902   134.000000     0.000000  \n",
       "50%      24.000000    36.695310     1.199951   137.000000     1.000000  \n",
       "75%      28.000000    38.195310     1.899902   141.000000     3.000000  \n",
       "max      90.000000    41.695310    21.500000   181.000000     7.073242  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441c8566-bc60-422a-b4a7-cdb7054fd711",
   "metadata": {},
   "source": [
    "\n",
    "I am particularly interested in determining whether to impute the missing values using the mean of each column or the median, or zero . In cases where most of the time the value is zero then I use the median otherwise I take the mean,  in the case of .\n",
    "\n",
    "\n",
    "* In case of \"scoma\" replace the NaNs with zero rather than the mean, we can see that in majority of cases the value is zero so the mean actually would not be represenative, indeed the median is zero.  \n",
    "* In the case of race I use a default the race to 1 when no race is provided. I chose to do this rather than use 0 or \"other\" as I believed that that wouled interfere less with the results given 1 is the most common value. \n",
    "* I replace NaNs of dnr to 0 where it is not provided (i.e that no DNR is provided)- we see that most values that are available are 0 and also logically that should be the default i.e no do not recussitate order.\n",
    "* In all other cases I replace the NANs with the mean of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "4166d5d4-daa3-4b56-999c-c7c6f348ebd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#X['dnr'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "b2912e53-6f91-44a8-8734-2f6a1b6bd3c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2t/dswlrlgs7r55b7gdz46sp2hc0000gn/T/ipykernel_94948/3915005101.py:8: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  X2=X2.fillna(X2.mean())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "dzgroup     0\n",
       "num.co      0\n",
       "scoma       0\n",
       "charges     0\n",
       "totcst      0\n",
       "avtisst     0\n",
       "race        0\n",
       "sps         0\n",
       "aps         0\n",
       "surv2m      0\n",
       "surv6m      0\n",
       "hday        0\n",
       "diabetes    0\n",
       "dementia    0\n",
       "ca          0\n",
       "dnr         0\n",
       "dnrday      0\n",
       "meanbp      0\n",
       "wblc        0\n",
       "hrt         0\n",
       "resp        0\n",
       "temp        0\n",
       "crea        0\n",
       "sod         0\n",
       "adlsc       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I remove dzclass as it's not independant to and is more general than dzgroup which is more specific\n",
    "X2 = X2.drop('dzclass', axis=1)\n",
    "\n",
    "\n",
    "#first replace NaNs in three columns using customised processes as appropriate\n",
    "X2 = X2.fillna({'scoma': 0, 'race':'white','dnr':'no dnr', 'ca':0 })\n",
    "#replace all other columns with column means\n",
    "X2=X2.fillna(X2.mean())\n",
    "\n",
    "#check that no NAs remain\n",
    "X2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "c6c4c5a2-0540-46b8-a542-81e7f42488dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age         0\n",
       "sex         0\n",
       "dzgroup     0\n",
       "num.co      0\n",
       "scoma       0\n",
       "charges     0\n",
       "totcst      0\n",
       "avtisst     0\n",
       "race        0\n",
       "sps         0\n",
       "aps         0\n",
       "surv2m      0\n",
       "surv6m      0\n",
       "hday        0\n",
       "diabetes    0\n",
       "dementia    0\n",
       "ca          0\n",
       "dnr         0\n",
       "dnrday      0\n",
       "meanbp      0\n",
       "wblc        0\n",
       "hrt         0\n",
       "resp        0\n",
       "temp        0\n",
       "crea        0\n",
       "sod         0\n",
       "adlsc       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X['ca'].describe()\n",
    "X2.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc874f6f-3cd4-4c99-a9ed-8810f69bbb83",
   "metadata": {},
   "source": [
    "### 2.2 Data Preprocessing: Transforming and Encoding Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9efe69-6d0e-4150-9725-812e33647fbf",
   "metadata": {},
   "source": [
    "Nomimal categories including sex (male or female), dzgroup (disease group) and race are each given one hot coding (also known as vector coding) accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "d62bea98-f10e-45f5-a395-9b9142f9d6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>num.co</th>\n",
       "      <th>scoma</th>\n",
       "      <th>charges</th>\n",
       "      <th>totcst</th>\n",
       "      <th>avtisst</th>\n",
       "      <th>sps</th>\n",
       "      <th>aps</th>\n",
       "      <th>surv2m</th>\n",
       "      <th>...</th>\n",
       "      <th>dzgroup_Cirrhosis</th>\n",
       "      <th>dzgroup_Colon Cancer</th>\n",
       "      <th>dzgroup_Coma</th>\n",
       "      <th>dzgroup_Lung Cancer</th>\n",
       "      <th>dzgroup_MOSF w/Malig</th>\n",
       "      <th>race_asian</th>\n",
       "      <th>race_black</th>\n",
       "      <th>race_hispanic</th>\n",
       "      <th>race_other</th>\n",
       "      <th>race_white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62.84998</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9715.0</td>\n",
       "      <td>30825.867768</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>33.898438</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.262939</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60.33899</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>44.0</td>\n",
       "      <td>34496.0</td>\n",
       "      <td>30825.867768</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>52.695312</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52.74698</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41094.0</td>\n",
       "      <td>30825.867768</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.790894</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.38498</td>\n",
       "      <td>female</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3075.0</td>\n",
       "      <td>30825.867768</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>20.097656</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.698975</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79.88495</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>50127.0</td>\n",
       "      <td>30825.867768</td>\n",
       "      <td>18.666656</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.634888</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9100</th>\n",
       "      <td>66.07300</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52870.0</td>\n",
       "      <td>34329.312500</td>\n",
       "      <td>20.333328</td>\n",
       "      <td>16.296875</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.852905</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>55.15399</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>35377.0</td>\n",
       "      <td>23558.500000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>25.796875</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.553955</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9102</th>\n",
       "      <td>70.38196</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46564.0</td>\n",
       "      <td>31409.015600</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>22.699219</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.741943</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9103</th>\n",
       "      <td>47.01999</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58439.0</td>\n",
       "      <td>30825.867768</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>40.195312</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.177979</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9104</th>\n",
       "      <td>81.53894</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15604.0</td>\n",
       "      <td>10605.757800</td>\n",
       "      <td>13.500000</td>\n",
       "      <td>18.097656</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.832886</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9105 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age     sex  num.co  scoma  charges        totcst    avtisst  \\\n",
       "0     62.84998    male       0    0.0   9715.0  30825.867768   7.000000   \n",
       "1     60.33899  female       2   44.0  34496.0  30825.867768  29.000000   \n",
       "2     52.74698  female       2    0.0  41094.0  30825.867768  13.000000   \n",
       "3     42.38498  female       2    0.0   3075.0  30825.867768   7.000000   \n",
       "4     79.88495  female       1   26.0  50127.0  30825.867768  18.666656   \n",
       "...        ...     ...     ...    ...      ...           ...        ...   \n",
       "9100  66.07300    male       1    0.0  52870.0  34329.312500  20.333328   \n",
       "9101  55.15399  female       1   41.0  35377.0  23558.500000  18.000000   \n",
       "9102  70.38196    male       1    0.0  46564.0  31409.015600  23.000000   \n",
       "9103  47.01999    male       1    0.0  58439.0  30825.867768  35.500000   \n",
       "9104  81.53894  female       1    0.0  15604.0  10605.757800  13.500000   \n",
       "\n",
       "            sps   aps    surv2m  ...  dzgroup_Cirrhosis  dzgroup_Colon Cancer  \\\n",
       "0     33.898438  20.0  0.262939  ...                  0                     0   \n",
       "1     52.695312  74.0  0.001000  ...                  1                     0   \n",
       "2     20.500000  45.0  0.790894  ...                  1                     0   \n",
       "3     20.097656  19.0  0.698975  ...                  0                     0   \n",
       "4     23.500000  30.0  0.634888  ...                  0                     0   \n",
       "...         ...   ...       ...  ...                ...                   ...   \n",
       "9100  16.296875  22.0  0.852905  ...                  0                     0   \n",
       "9101  25.796875  31.0  0.553955  ...                  0                     0   \n",
       "9102  22.699219  39.0  0.741943  ...                  0                     0   \n",
       "9103  40.195312  51.0  0.177979  ...                  0                     0   \n",
       "9104  18.097656   7.0  0.832886  ...                  0                     0   \n",
       "\n",
       "      dzgroup_Coma  dzgroup_Lung Cancer dzgroup_MOSF w/Malig race_asian  \\\n",
       "0                0                    1                    0          0   \n",
       "1                0                    0                    0          0   \n",
       "2                0                    0                    0          0   \n",
       "3                0                    1                    0          0   \n",
       "4                0                    0                    0          0   \n",
       "...            ...                  ...                  ...        ...   \n",
       "9100             0                    0                    0          0   \n",
       "9101             1                    0                    0          0   \n",
       "9102             0                    0                    0          0   \n",
       "9103             0                    0                    1          0   \n",
       "9104             0                    0                    0          0   \n",
       "\n",
       "      race_black  race_hispanic  race_other  race_white  \n",
       "0              0              0           1           0  \n",
       "1              0              0           0           1  \n",
       "2              0              0           0           1  \n",
       "3              0              0           0           1  \n",
       "4              0              0           0           1  \n",
       "...          ...            ...         ...         ...  \n",
       "9100           0              0           0           1  \n",
       "9101           0              0           0           1  \n",
       "9102           0              0           0           1  \n",
       "9103           0              0           0           1  \n",
       "9104           0              0           0           1  \n",
       "\n",
       "[9105 rows x 38 columns]"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nominal categories\n",
    "#X2['sex']=X2['sex'].astype('str').map({'female': 1, 'male':0})\n",
    "#X2['dzgroup']=X2['dzgroup'].astype('str').map({'Lung Cancer': 0, 'Cirrhosis':1, 'ARF/MOSF w/Sepsis': 2, 'Coma':3,'CHF':4 , 'Colon Cancer':5,'COPD':6,'MOSF w/Malig':7 })\n",
    "#X2['race']=X2['race'].astype('str').map({'other': 0, 'white':1, 'black': 2, 'hispanic':3,'asian':4})\n",
    "\n",
    "\n",
    "# Get one hot encoding of nominal category columns\n",
    "one_hot = pd.get_dummies(X2[['dzgroup','race']])\n",
    "one_hot\n",
    "\n",
    "# Drop encoded columns as they are each now encoded\n",
    "X2 = X2.drop(['dzgroup','race'], axis=1)\n",
    "\n",
    "# Join the encoded df\n",
    "X2 = X2.join(one_hot)\n",
    "X2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f3bb27-a737-45ef-94cf-c62fd43c59d4",
   "metadata": {},
   "source": [
    "Ordinal categories are encoded accordinly below including the dnr (do not recussitate category) encoded in order of no DNR, DNR before sadm (i.e before admission), DNR after sadm (i.e after admission).\n",
    "\"ca\" cancer category is encoded according to \"no\" no cancer (0), yes or cancer present (1), \"metastatic\" or cancer has spread (2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "01ddc743-b1dc-4204-9d73-0d7b12fd6c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#we observe that after the NAs are removed that there are many categorical variables remaining that need to be encoded\n",
    "#it is necessary to do this before imputing the NAs as that will directly impact the imputation process\n",
    "\n",
    "\n",
    "#ordinal categories\n",
    "#from \"no\" as in no cancer, cancer present, to mestastatic (cancer has metatasised and spread)\n",
    "#X2['ca']=X2['ca'].astype('str').map({'no':0, 'yes':1,'metastatic': 2})\n",
    "X2['ca']=X2['ca'].map({'no':0, 'yes':1,'metastatic': 2})\n",
    "\n",
    "#no dnr, then dnr before sadm, then dnr after sadm\n",
    "#X2['dnr']=X2['dnr'].astype('str').map({'no dnr': 0,  'dnr before sadm':1,'dnr after sadm':2})\n",
    "X2['dnr']=X2['dnr'].map({'no dnr': 0,  'dnr before sadm':1,'dnr after sadm':2})\n",
    "\n",
    "X2['sex']=X2['sex'].map({'male':0, 'female':1})\n",
    "\n",
    "\n",
    "#I transform charges in USD to k_USD and convrt to integer to make the results more interpretable\n",
    "#it's noted although the costs are reported to 1 dp the dp is always zero and in any case would not likely materially affect any conclusions drawn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "0ec50446-6d4f-491b-bbe3-5e4315b02eea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1])"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2['ca'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e7bd95-1dad-42c7-8f45-b1647ac7d04b",
   "metadata": {},
   "source": [
    "I remove dzclass since it's not independant to and is more general than dzgroup which is more specific.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "35e7da0b-f593-4c19-8cff-f5105780ba13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1])"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2['dnr'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "498ee49d-d721-42a6-a95c-a4f749bf4f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#converting hospital charges from USD to thousand USD and rounding to whole number\n",
    "#X2['charges']=X2['charges']/1000\n",
    "#X2['charges']=X2['charges'].astype(int)\n",
    "#X2['charges']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d987cb4-a24d-4db5-935f-81ef248d9935",
   "metadata": {},
   "source": [
    "### 2.5 Data Preprocessing: Splitting data into test and training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "d0571ea7-c749-4bb3-a709-0d064d6ae2bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X2,Y, test_size=0.2, random_state=3)\n",
    "\n",
    "#Y=y[['hospdead']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64fcc7a-86ec-470a-b73d-6fe8ac9a45ac",
   "metadata": {},
   "source": [
    "### 2.6 Data Preprocessing: Checking for class imbalance\n",
    "We observe a clear class imbalance with 74% of the dependant variable labelled 0 (i.e survived) and remainder 26% only represent hospital deaths. This has implications for model evaluation- should be using micro precision and micro recall for instance in order to account for such an imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "f65a1c58-36e6-44cd-aafa-dd9123aeb026",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hospdead\n",
       "0           0.741214\n",
       "1           0.258786\n",
       "dtype: float64"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for distribution of labels\n",
    "\n",
    "Y_train.value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da1df55-d406-4731-be6c-7e51f4bf5aa5",
   "metadata": {},
   "source": [
    "## 3 Preliminary Model 1 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d49e28-2d2d-4cfc-8662-b86f0cbe220d",
   "metadata": {},
   "source": [
    "### 3.1 Preliminary Model 1 Implementation: Logistic Regression Model Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64626817-1c91-4654-ad47-124fc66a0859",
   "metadata": {},
   "source": [
    "My objective here is first to identify potentially useful explanatory variables and then to carry out exploratory analysis, including particularly plots of the features to check for independance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "e60a2b62-2add-43e8-ae8e-80998571ecec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neilwatt/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/neilwatt/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the linear regression model\n",
    "\n",
    "\n",
    "model_log = LogisticRegression()\n",
    "logistic_fit=model_log.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "Y_pred_log = np.round(model_log.predict(X_test)) # rounded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "3ea3b9d2-9cf3-4c02-a69d-386ab0f9a1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.254149\n",
      "         Iterations 8\n",
      "                          MNLogit Regression Results                          \n",
      "==============================================================================\n",
      "Dep. Variable:               hospdead   No. Observations:                 7284\n",
      "Model:                        MNLogit   Df Residuals:                     7247\n",
      "Method:                           MLE   Df Model:                           36\n",
      "Date:                Sat, 20 Jul 2024   Pseudo R-squ.:                  0.5555\n",
      "Time:                        23:14:10   Log-Likelihood:                -1851.2\n",
      "converged:                       True   LL-Null:                       -4164.9\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "=============================================================================================\n",
      "               hospdead=1       coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------\n",
      "age                           0.0121      0.005      2.655      0.008       0.003       0.021\n",
      "sex                          -0.0454      0.087     -0.524      0.600      -0.215       0.124\n",
      "num.co                       -0.0542      0.040     -1.350      0.177      -0.133       0.025\n",
      "scoma                         0.0102      0.003      3.193      0.001       0.004       0.016\n",
      "charges                   -1.772e-07   5.54e-07     -0.320      0.749   -1.26e-06    9.08e-07\n",
      "totcst                    -2.989e-06   1.37e-06     -2.184      0.029   -5.67e-06   -3.07e-07\n",
      "avtisst                       0.1107      0.005     22.141      0.000       0.101       0.120\n",
      "sps                           0.0435      0.015      2.842      0.004       0.014       0.074\n",
      "aps                           0.0174      0.004      4.490      0.000       0.010       0.025\n",
      "surv2m                       -0.5419      1.271     -0.426      0.670      -3.034       1.950\n",
      "surv6m                        0.3542      1.146      0.309      0.757      -1.892       2.600\n",
      "hday                          0.0162      0.005      3.468      0.001       0.007       0.025\n",
      "diabetes                     -0.1147      0.122     -0.942      0.346      -0.353       0.124\n",
      "dementia                     -0.1049      0.209     -0.501      0.617      -0.515       0.306\n",
      "ca                            0.0736      0.111      0.664      0.507      -0.144       0.291\n",
      "dnr                           1.4003      0.051     27.549      0.000       1.301       1.500\n",
      "dnrday                       -0.0092      0.003     -3.196      0.001      -0.015      -0.004\n",
      "meanbp                        0.0007      0.002      0.445      0.656      -0.002       0.004\n",
      "wblc                         -0.0010      0.005     -0.212      0.832      -0.010       0.008\n",
      "hrt                           0.0010      0.001      0.686      0.493      -0.002       0.004\n",
      "resp                         -0.0035      0.004     -0.814      0.415      -0.012       0.005\n",
      "temp                         -0.0266      0.036     -0.749      0.454      -0.096       0.043\n",
      "crea                          0.0035      0.025      0.142      0.887      -0.045       0.053\n",
      "sod                          -0.0006      0.007     -0.093      0.926      -0.014       0.013\n",
      "adlsc                         0.0615      0.021      2.926      0.003       0.020       0.103\n",
      "dzgroup_ARF/MOSF w/Sepsis    -7.0882      1.880     -3.769      0.000     -10.774      -3.403\n",
      "dzgroup_CHF                  -6.9839      1.873     -3.728      0.000     -10.655      -3.313\n",
      "dzgroup_COPD                 -7.1815      1.876     -3.829      0.000     -10.858      -3.505\n",
      "dzgroup_Cirrhosis            -6.3293      1.832     -3.455      0.001      -9.920      -2.739\n",
      "dzgroup_Colon Cancer         -6.7553      1.905     -3.546      0.000     -10.489      -3.021\n",
      "dzgroup_Coma                 -6.3587      1.828     -3.479      0.001      -9.941      -2.776\n",
      "dzgroup_Lung Cancer          -6.3249      1.843     -3.431      0.001      -9.938      -2.712\n",
      "dzgroup_MOSF w/Malig         -6.7953      1.847     -3.679      0.000     -10.415      -3.175\n",
      "race_asian                    0.6932      0.423      1.640      0.101      -0.135       1.522\n",
      "race_black                    0.2660      0.123      2.158      0.031       0.024       0.508\n",
      "race_hispanic                 0.0515      0.269      0.191      0.848      -0.475       0.578\n",
      "race_other                    0.2102      0.384      0.547      0.584      -0.543       0.963\n",
      "=============================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Note that since the logistic model in sci learn does not give ability \n",
    "#to extra p values for coefficients I created a M logistic model accordingly\n",
    "\n",
    "import statsmodels.api as smodel\n",
    "def SWEEPOperator (pDim, inputM, origDiag, sweepCol = None, tol = 1e-7):\n",
    "    ''' Implement the SWEEP operator\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    pDim: dimension of matrix inputM, integer greater than one\n",
    "    inputM: a square and symmetric matrix, numpy array\n",
    "    origDiag: the original diagonal elements before any SWEEPing\n",
    "    sweepCol: a list of columns numbers to SWEEP\n",
    "    tol: singularity tolerance, positive real\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A: negative of a generalized inverse of input matrix\n",
    "    aliasParam: a list of aliased rows/columns in input matrix\n",
    "    nonAliasParam: a list of non-aliased rows/columns in input matrix\n",
    "    '''\n",
    "\n",
    "    if (sweepCol is None):\n",
    "        sweepCol = range(pDim)\n",
    "\n",
    "    aliasParam = []\n",
    "    nonAliasParam = []\n",
    "\n",
    "    A = np.copy(inputM)\n",
    "    ANext = np.zeros((pDim,pDim))\n",
    "\n",
    "    for k in sweepCol:\n",
    "        Akk = A[k,k]\n",
    "        pivot = tol * abs(origDiag[k])\n",
    "        if (not np.isinf(Akk) and abs(Akk) >= pivot):\n",
    "            nonAliasParam.append(k)\n",
    "            ANext = A - np.outer(A[:, k], A[k, :]) / Akk\n",
    "            ANext[:, k] = A[:, k] / abs(Akk)\n",
    "            ANext[k, :] = ANext[:, k]\n",
    "            ANext[k, k] = -1.0 / Akk\n",
    "        else:\n",
    "            aliasParam.append(k)\n",
    "            ANext[:,k] = np.zeros(pDim)\n",
    "            ANext[k, :] = np.zeros(pDim)\n",
    "        A = ANext\n",
    "    return (A, aliasParam, nonAliasParam)\n",
    "\n",
    "def MNLogisticModel (X, y, maxIter = 20, tolSweep = 1e-7):\n",
    "    ''' Train a Multinomial Logistic Model\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    X: A Pandas DataFrame, rows are observations, columns are regressors\n",
    "    y: A Pandas Series, rows are observations of the response variable\n",
    "    maxIter: Maximum number of iterations\n",
    "    tolSweep: Tolerance for SWEEP Operator\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    A list of model output:\n",
    "    (0) mFit: the Fit object of MNLogit\n",
    "    (1) mLLK: model log-likelihood value\n",
    "    (2) mDF: model degrees of freedom\n",
    "    (3) mParameter: model parameter estimates5\n",
    "    (4) aliasParam: indices of aliased parameters\n",
    "    (5) nonAliasParam: indices of non-aliased parameters\n",
    "    '''\n",
    "\n",
    "    n_param = X.shape[1]\n",
    "\n",
    "    # Identify the aliased parameters\n",
    "    XtX = X.transpose().dot(X)\n",
    "    origDiag = np.diag(XtX)\n",
    "    XtXGinv, aliasParam, nonAliasParam = SWEEPOperator (n_param, XtX, origDiag, sweepCol = range(n_param), tol = tolSweep)\n",
    "\n",
    "    # Train a multinominal logistic model\n",
    "    X_reduce = X.iloc[:, list(nonAliasParam)]\n",
    "    mObj = smodel.MNLogit(y, X_reduce)\n",
    "    mFit = mObj.fit(method = 'newton', maxiter = maxIter, tol = 1e-6, full_output = True, disp = True)\n",
    "    mLLK = mFit.llf\n",
    "    mDF = len(nonAliasParam) * (mFit.J - 1)\n",
    "    mParameter = mFit.params\n",
    "\n",
    "    # Return model statistics\n",
    "    return ([mFit, mLLK, mDF, mParameter, aliasParam, nonAliasParam])\n",
    "\n",
    "Model_1 = MNLogisticModel (X_train, Y_train)\n",
    "\n",
    "\n",
    "print(Model_1[0].summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9814ea00-833f-41f7-bcec-c7964641a6fa",
   "metadata": {},
   "source": [
    "Looking at the fit above we observe the following:\n",
    "* the p-value for the model as a whole is 0.0 which suggests the model is statistically signifcant to the 95% confidence\n",
    "\n",
    "In addition the following coefficients were found to be statistically significant to the 95% confidence observing their respective p-values:\n",
    "\n",
    "\n",
    "\n",
    "* age\n",
    "* scoma, Coma Score based on Glasgow scale.\n",
    "* totcst or the total ratio of costs to charges (RCC) cost.\n",
    "* avtisst, average TIIS score over days 3-25. Therapeutic Intervention Scoring System (TISS) is a method for calculating costs in the intensive care unit (ICU) and intermediate care unit (IMCU). TISS essentially quantifies the type and number of care treatments. (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3333165/)\n",
    "* sps (physiology score on day 3)\n",
    "* aps. The dataset reference describes aps as \"APACHE III day 3 physiology score (no coma, imp bun,uout for ph1). This is essentially a score whose purpose is to predict mortality of patients admitted to ICU, the score takes into consideration age, level of conciousness and various physiological factors. Worth consiering at this point that there is a possibility this feature may not be completely independant from the others given how it is calculated. (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4893757/#:~:text=APACHE%20III%20is%20widely%20used,into%20the%20score%20(18))\n",
    "* hday. Day in hospital at which patient entered the study.\n",
    "* dnr, whether the patient has a do not rescuscitate order or not.\n",
    "* dnrday\n",
    "* adlsc, imputed ADL calibrated to surrogate. ADL is an acronym for \"Activities of Daily Living\" and is a term used that describes basic everyday skills that are essential to living independantly. (https://www.physio-pedia.com/Activities_of_Daily_Living)\n",
    "* dzgroup, all categories are relevant (dzgroup_ARF/MOSF w/Sepsis, dzgroup_CHF, dzgroup_COPD, dzgroup_Cirrhosis, dzgroup_Colon Cancer, dzgroup_Coma, dzgroup_Lung Cancer, dzgroup_MOSF w/Malig).          \n",
    "* race (race_black and race_1 only). This is clearly something that needs to be treated extremely sensitively with respect to any inferences that might be drawn. \n",
    "\n",
    "\n",
    "\n",
    "After the data exploration phase I will re-fit the model including only those coefficients that have p-values less than 0.05 for which we fail to reject the null hypothesis at the 95% confidence level. Before re-fitting the model I will first evaluate Model 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "96d1a883-5ec4-4c78-bfa6-7677891ca97a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='ca'>"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fig, ax =plt.subplots(1,1,figsize=(20,10))\n",
    "#sns.stripplot(y=Y['hospdead'], x=X2['ca'])\n",
    "\n",
    "pd.crosstab(X2['ca'],Y['hospdead'],normalize='index').plot.bar(stacked=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f38f5-c0dd-4426-92d8-41bdafb0ed6d",
   "metadata": {},
   "source": [
    "### 3.2 Preliminary Model 1 Implementation Implementation: Logistic Regression Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "8f25b996-b620-46b3-8c1f-17cadf9fd5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model: 0.85\n",
      "Micro Precision: 0.8511806699615596\n",
      "Micro Recall: 0.8511806699615596\n",
      "r2: 0.2281363885195904\n",
      "mse: 0.14881933003844042\n",
      "bic: 4031.5013764850682\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test,  Y_pred_log)\n",
    "print(f\"Accuracy of the model: {accuracy:.2f}\")\n",
    "\n",
    "precision_micro = precision_score(Y_test, Y_pred_log, average='micro')\n",
    "recall_micro = recall_score(Y_test,Y_pred_log, average='micro')\n",
    "r2 = r2_score(Y_test, Y_pred_log)\n",
    "mse = mean_squared_error(Y_test, Y_pred_log)\n",
    "\n",
    "\n",
    "\n",
    "print(f'Micro Precision: {precision_micro}')\n",
    "print(f'Micro Recall: {recall_micro}')\n",
    "print(f'r2: {r2}')\n",
    "print(f'mse: {mse}')\n",
    "print(f'bic: {Model_1[0].bic}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display confusion matrix\n",
    "confusion_matrix = pd.crosstab(Y_test['hospdead'], Y_pred_log.flatten(), rownames=['Actual'], colnames=['Predicted'])\n",
    "sn.heatmap(confusion_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b6110-6500-437d-b9aa-14c3e708194a",
   "metadata": {},
   "source": [
    "I find above that Model 1 has an accuracy of 0.83, relatively high. In addition the micro precision and recall each at 0.834 also relatively high. Since we have a class imbalance (insofar as there are much more patients that do not die than do) I am using micro as opposed to the other measures (i.e no weighted or macro etc).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22615014-439f-4ec8-b9eb-a49b73001702",
   "metadata": {},
   "source": [
    "## 4 Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf81ec-7a5b-4e7e-88d0-e30c514e2603",
   "metadata": {},
   "source": [
    "My main aim here was to determine whether the features used were actually independant, also to uncover any interesting interactions or relationships that were not earlier apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "f347d62e-0381-40ed-a930-3d34cf7d7856",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['dzgroup', 'race'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[422], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#create new dataframe including only the coefficients that were statistically sig. in Model 1\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m X3\u001b[38;5;241m=\u001b[39m\u001b[43mX2\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdzgroup\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscoma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcharges\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotcst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavtisst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrace\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhday\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdementia\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdnr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43madlsc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      4\u001b[0m X3\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:3464\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3462\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3463\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3464\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_listlike_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3466\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1314\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1312\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 1314\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_read_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m needs_i8_conversion(ax\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1317\u001b[0m     ax, (IntervalIndex, CategoricalIndex)\n\u001b[1;32m   1318\u001b[0m ):\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;66;03m# For CategoricalIndex take instead of reindex to preserve dtype.\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;66;03m#  For IntervalIndex this is to map integers to the Intervals they match to.\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m     keyarr \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mtake(indexer)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1377\u001b[0m, in \u001b[0;36m_LocIndexer._validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1376\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 1377\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['dzgroup', 'race'] not in index\""
     ]
    }
   ],
   "source": [
    "#create new dataframe including only the coefficients that were statistically sig. in Model 1\n",
    "\n",
    "X3=X2[['dzgroup','scoma','charges','totcst','avtisst','race','aps','hday','dementia','dnr','temp','adlsc']]\n",
    "X3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c1b165-fabc-4b6d-b390-5000930579cc",
   "metadata": {},
   "source": [
    "#### 4.1 Data Exploration: Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b193fa5-d1c6-4f31-bafb-c9bca3efbfd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create correlation matrix to help check for independance of coefficients\n",
    "\n",
    "\n",
    "corr = X3.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm').set_precision(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379295d-95ed-4b26-a1ad-8f530ac37d07",
   "metadata": {},
   "source": [
    "From the heatmap/correlation matrix above there are several observations:\n",
    "* totcst (total ratio of costs to charges (RCC) cost) has a high correlation (0.77) with charges (hospital charges). I postulate that this is liklely due to the fact that, all other things being equal, the more medical interventions that have taken place; the more the hospital will charge the patient. \n",
    "* there is a 0.59 correlation between aps (APACHE III day 3 physiology score) and avtisst (average  Therapeutic Intervention Scoring System). This correlation may be in some way related to the nature of the aps reflecting the severity of a patient's condition and, presumably, the more severe a patient's condition the more clinical interventions are required.\n",
    "* It is noted that there is a signficant correlation between hday (day in hospital at which patient entered study) and charges at 0.47. This relationship seems spurious as the longer before a patient has been admitted to a study the more treatments it's likely they have received and the more that patient has been charged.\n",
    "* There is a small but signifcant correlation 0.46 between hday (day in hospital at which patient entered study) and totcst (total ratio of costs to charges (RCC) cost). \n",
    "* There is a 0.46 correlation between avtisst (average  Therapeutic Intervention Scoring System) and totcst (total ratio of costs to charges (RCC) cost). \n",
    "\n",
    "At this point I decide to drop hday since it is correlated to both totcst and charges. In addition hday also appears to be a spurious value insofar as it's clearly more likely the longer before someone is admitted to the study (having not recovered and been released) then naturally the more likely it is that their costs would be higher and that they may have been admitted multiple times.\n",
    "\n",
    "I consider also the other interactions in the scatterplots below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f4ac0e-1fa2-4ae9-b638-f958f37a49b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping column hday (days before patient admitted to study)\n",
    "#at this point I disregard the relationships between hday and charges, hday and totcst\n",
    "X4=X3\n",
    "X4 = X4.drop('hday', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53408e98-ce20-4c35-90fe-2f55c9b969b1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.2 Data Exploration: Scatterplots of Related Independant Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "52ba2953-cdda-4a0d-a60f-cfeeb9e45ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X4' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[443], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#considering first the 0.77 correlation between total ratio of costs to \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#charges cost (totcst) and charges\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#and we also see generally a diagonal patter suggesting a casual relationship between the two variables\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#I decide at this point to remove charges altogether as a training variable\u001b[39;00m\n\u001b[1;32m     10\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39mplt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m---> 12\u001b[0m sns\u001b[38;5;241m.\u001b[39mstripplot(x\u001b[38;5;241m=\u001b[39mY[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhospdead\u001b[39m\u001b[38;5;124m'\u001b[39m], y\u001b[38;5;241m=\u001b[39m\u001b[43mX4\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotcst\u001b[39m\u001b[38;5;124m'\u001b[39m],ax\u001b[38;5;241m=\u001b[39max[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     13\u001b[0m sns\u001b[38;5;241m.\u001b[39mstripplot(x\u001b[38;5;241m=\u001b[39mY[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhospdead\u001b[39m\u001b[38;5;124m'\u001b[39m], y\u001b[38;5;241m=\u001b[39mX4[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharges\u001b[39m\u001b[38;5;124m'\u001b[39m], ax\u001b[38;5;241m=\u001b[39max[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X4' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#considering first the 0.77 correlation between total ratio of costs to \n",
    "#charges cost (totcst) and charges\n",
    "\n",
    "#we can observe from the two charts below which each compare hopsdeads to totcst and charges \n",
    "#that totcst has clearer separation between survival and death so is preferred out of the two features\n",
    "#reviewing the chart on the right hand side (totcst and charges) we see some linear patters at certain levels \n",
    "#and we also see generally a diagonal patter suggesting a casual relationship between the two variables\n",
    "#I decide at this point to remove charges altogether as a training variable\n",
    "\n",
    "fig, ax =plt.subplots(1,2,figsize=(20,10))\n",
    "\n",
    "sns.stripplot(x=Y['hospdead'], y=X4['totcst'],ax=ax[0])\n",
    "sns.stripplot(x=Y['hospdead'], y=X4['charges'], ax=ax[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7303d9c9-e928-43b5-b666-b7ae724e38c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#reviewing the chart below (totcst and charges) we see some linear patters at certain levels \n",
    "#and we also see generally a diagonal patter suggesting a casual relationship between the two variables\n",
    "#I decide at this point to remove charges altogether as a training variable\n",
    "\n",
    "\n",
    "ax=sns.scatterplot( x=X4['totcst'], y=X4['charges'],hue=Y['hospdead'])\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "ax.set(title=\"Plot of total ratio of costs to charges cost (totcst) and charges\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c5327e-354c-461b-8d6d-2eb0a63850c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#remove charges from X4\n",
    "X4 = X4.drop('charges', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c48ca1e-3cce-41e5-aa92-9e387fb7c2da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#considering the 0.59 correlation between aps and avtisst\n",
    "\n",
    "#we observe that there is broadly a widely dispersed diagonal pattern which suggests somewhat of a relationship between aps and avtisst\n",
    "#we see however that there is a much clearer separation between hosptial deaths from left to right (i.e with increasing aps) \n",
    "#than with increasing avtisst\n",
    "# it is decided at this point to consider later disregarding avtisst to see whether it improves results (i.e experiment with retaining and excluding it)\n",
    "\n",
    "ax = sns.stripplot(x=X4['aps'], y=X4['avtisst'],hue=Y['hospdead'])\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "ax.xaxis.set_major_locator(ticker.LinearLocator(10))\n",
    "ax.set(title=\"Plot of average TIIS score over days 3-25 (aps) and APACHE III day 3 physiology score and (avtisst)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6314998a-0be7-4122-b5bd-f9ddfda36347",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#considering the 0.46 correlation between avtisst and totcst\n",
    "#we see a clearer separation of hospital deaths on the x-axis according to avtisst than we according to totcst\n",
    "#given that it was considered to remove avtisst (given it's relationship with aps and less effective separation of hospital deaths)\n",
    "#it is decided to retain totcst in cases where avtisst has been removed \n",
    "\n",
    "\n",
    "ax=sns.stripplot(x=X4['avtisst'].round(), y=X4['totcst'],hue=Y['hospdead'])\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "ax.xaxis.set_major_locator(ticker.LinearLocator(5))\n",
    "ax.set(title=\"Plot of APACHE III day 3 physiology score and (avtisst) and total ratio of costs to charges cost(totcst)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ddc0a6-7904-4b6b-bb2b-df739018c454",
   "metadata": {},
   "source": [
    "To summarise the main conclusions from the exploratory analysis:\n",
    "* Was determined to exclude hospital charges which had a 0.77 correlation with totcst. There is a very clear diagonal relationship between the two generally and also some other clearly linear patterns which would violate the independance assumptions, charges was found to be not as good a separator of hospital deaths vs survival as totcst.\n",
    "* It may make sense to exclude APACHE III day 3 physiology score and (avtisst) given it's high correlation (0.59) with aps but less clear separation of hospital deaths than aps. Will experiement with including it (X4, model 2) and excluding it (X6, model 3).\n",
    "* It may also make sense to exclude  totcst given it's apparent relationship with avtisst that would only make sense in cases where we are including avtisst."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d44bc8-402e-43f7-8e06-8f2c4d7b5fb9",
   "metadata": {},
   "source": [
    "#### 4.3 Data Exploration: Correlation between Features and Dependant Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48418a44-16c8-4a0a-8832-b2aaeadf6919",
   "metadata": {
    "tags": []
   },
   "source": [
    "Below I consider the correlations between the Y variable hospital deaths () and the feature variables in X4. It can be noted from the below that:\n",
    "* scoma, totcst, aps and dnr have substantial correlations with hospital deaths (hospdead) and the largest of the group\n",
    "* race, dementia, temp and adlsc have the smallest correlations. Although these factors have the lowest correlations it does not necessarily mean they are insignficant as for example it may be the case that some of the race categories that rarely appear are signficant indicators of higher chance of mortality. Equally it may be case that dementia is rare but when it does appear is a signficant indicator.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f3852-7624-42be-a3b0-816c25186d3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for purposes of the correlation matrix I add the dependant variable\n",
    "X_correlation=X4.copy()\n",
    "X_correlation['hospdead']=Y['hospdead']\n",
    "\n",
    "X_correlation.corr()['hospdead']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a2804-f189-4c4a-9fbd-a33ccd92d206",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4.4 Data Exploration: Boxplots of features and dependant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d2dd7e-e6bc-44c2-8fc6-0f49ce0d180e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#with hospital death/survival as x\n",
    "for column in X_correlation.columns[0:]:  # Loop over all columns except 'Location'\n",
    "    sns.set()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    sns.set(style=\"ticks\")\n",
    "    sns.boxplot(x='hospdead', y=column, data=X_correlation)  # column is chosen here\n",
    "    sns.despine(offset=10, trim=True) \n",
    "    \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690790b-5675-4fe5-a969-3c5c91d1217f",
   "metadata": {},
   "source": [
    "I make the following observations from the boxplots above:\n",
    "* There is a clear relationship between scoma and hospital deaths. It is notable that the median level for those patients who survive has no coma but that there are  a number of outliers where a patient goes into comma and survives but this seems not to be the norm.\n",
    "\n",
    "* The relationship between totcst and hospital death does not seem to be clear. On the one hand we see that the median totcst value for hospital deaths being positive is higher than for the survivors we see on the other hand that there is a wide spread and in fact far higher values for totcst in the survivors in many cases, so the relationship is far from straight forward. It is suggested that it is unclear how useful totcst is.\n",
    "\n",
    "* There is a clear relationship between avtisst and hospital deaths with higher avtisst associated with hospital deaths. We see not only that the median avtisst values are higher in the case of the hospital deaths but that also the maximum values of avtisst in case of hospital deaths are far exceeding that of the survivors (despite the clear class imbalance that I touched on earlier).\n",
    "\n",
    "* There is no clear relationship with race that can be observed from the box plot, what we really need to plot instead is the percentage by category given we are looking at categorical variables.\n",
    "\n",
    "* There is a very clear relationship between aps and hospital deaths with higher aps being associated with hospital deaths, not only the median being higher but also the maximum values notably higher in the case of hospital deaths.\n",
    "\n",
    "* There is no clear relationship with dementia that can be observed from the box plot, what we really need to plot instead is the percentage by category given we are looking at categorical variables as with race.\n",
    "\n",
    "* There is an apparent positive relationship  between dnr and hospital deaths with the median dnr being higher in the case of hospital deaths.\n",
    "\n",
    "* We see that the adlsc has an apparent positive relationship with hospital deaths with higher median (denoted by the horizontal line).\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3d4abd-4b46-4a41-b6be-f022a861844c",
   "metadata": {},
   "source": [
    "#### 4.5 Data Exploration: Scatterplots of features and dependant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be31624e-9bc5-4a3f-a165-45147f74ebbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#sns.pairplot(X_correlation, X_correlation['hospdead'])\n",
    "#ax.set(title=\"Plot of Features \")\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#f, ax_l = plt.subplots(1, 10, figsize=(14, 4))\n",
    "\n",
    "f, ax_l = plt.subplots(10,1, figsize=(6, 28))\n",
    "#fig, axs = plt.subplots(2, 2)\n",
    "#f, ax_l =plt.subplots(2, 5)\n",
    "for e, col_name in enumerate(X_correlation.loc[:,:'adlsc' ].columns):\n",
    "    ax_l[e].scatter( X_correlation[col_name], X_correlation.hospdead,alpha=0.05, color='r')\n",
    "    ax_l[e].set_xlabel(col_name)\n",
    "    ax_l[e].set_ylabel('hospdead')\n",
    "    ax_l[e].xaxis.set_major_locator(ticker.LinearLocator(2))\n",
    "    \n",
    "plt.tight_layout() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f850997a-650b-41ab-9952-18e142e190e3",
   "metadata": {},
   "source": [
    "We observe in the above scatter charts:\n",
    "* no clear relationship between dzgroup and hospital deaths, better to do a count by category chart.\n",
    "* Likewise no clear relationship between coma and hospital deaths, better to do a count by category chart.\n",
    "* Relationship between hospital deaths and costs is not clear. We see a few datapoints that show survial at high cost whereas before we know that the coefficient was positive in the earlier logistic regression. Consider removing totcst.\n",
    "* There is a clear relationship between avtisst and hospital deaths in the scatterplot.\n",
    "* Unclear relationship between race and hospital deaths as earlier will follow up with a count by category chart.\n",
    "* Clear relationship between aps and hospital deaths with higher deaths for higher aps\n",
    "* Unclear relationship between dementia and hospital deaths as earlier will follow up with a count by category chart.\n",
    "* Unclear relationship between dnr and hospital deaths as earlier will follow up with a count by category chart.\n",
    "* Temperature appears to be almost symetric about hospital deaths but with a slight bias towards surviving at lower temperatures and death at higher temperatures but this is not a linear relationship it is more the case that at the extremes it is signficant. Consider removing temperature or transforming appropriately.\n",
    "* Relationship between adlsc is unclear there is a large number of high adlsc datapoints that have surviving patients whereas the coefficient before was positve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048d5e4-1294-4093-b2e2-e1eca5186770",
   "metadata": {},
   "source": [
    "#### 4.6 Data Exploration: Countplots select features again dependant variable "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e8a91b-d2a1-4589-bff1-9ae02d6d3e55",
   "metadata": {},
   "source": [
    "Here I carry out percent countplots on the categorical variables race, dnr and dementia against the dependant variables. This is done as neither the boxplots nor the scatterplots provided a clear interpretation of any possible relationship, or lackof."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162faac-0a3c-44e1-88a5-c0a85e4d6a51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#since there were 7 classes of disease wanted to see if relationship clearer as countplot\n",
    "#it is observed that there are clearly certain strong indications of relationships between some of the categorical values of dzgroup\n",
    "# and hospital death/survival\n",
    "#in order of relation with death:  3, 7, 2,1,0,6,4,5 I decide to remap accordingly\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.crosstab(X_correlation['dzgroup'],X_correlation['hospdead'],normalize='index').plot.bar(stacked=True)\n",
    "X2['dzgroup']=X2['dzgroup'].astype('str').map({'Lung Cancer': 0, 'Cirrhosis':1, 'ARF/MOSF w/Sepsis': 2, 'Coma':3,'CHF':4 , 'Colon Cancer':5,'COPD':6,'MOSF w/Malig':7 })\n",
    "\n",
    "X4['dnr']=X4['dnr'].map({0: 0, 2:1, 1:2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc33208-0bfa-4ced-a9ad-034291e0230f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plotting percentage appearance of race categories and hospital deaths\n",
    "#observe that proportion mortality in descending order proceeds as:\n",
    "# 4 (asian), 0 (other), 2 (black), 1 (white), 3 (hispanic)\n",
    "#would postulate here that there may also be some relationship between age other factors and the age of patients admitted so I do another chart below\n",
    "#I decide to remap accordingly \n",
    "\n",
    "pd.crosstab(X_correlation['race'],X_correlation['hospdead'],normalize='index').plot.bar(stacked=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d08ae1-d34a-46f6-a128-cb6e9526e4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#remapped race according to mortality\n",
    "#  (asian 0),  (other 1), (black 2), (white 3),  (hispanic4)\n",
    "X4['race']=X4['race'].map({4: 0, 0:1, 2:2, 1:3, 3:4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff3ca45-f136-43b3-aebb-f014627c7ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#countplot (percent) of dnr and hospital deaths\n",
    "# it is clearly apparent that when dnr is 1 that it is clearly related to hospital deaths (represents DNR after admission)\n",
    "#the relationship between dnr before admission is less clear but more substantial than the no dnr\n",
    "#determine to remap as no dnr: 0, dnr before admission 1, dnr after admission 2\n",
    "\n",
    "\n",
    "pd.crosstab(X_correlation['dnr'],X_correlation['hospdead'],normalize='index').plot.bar(stacked=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd910b-f34d-4605-9f32-d9d9581272f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#we see that dnr bef\n",
    "X4['dnr']=X4['dnr'].map({0: 0, 2:1, 1:2})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5e559c-7462-4267-b228-5e2a940e0876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#countplot of dementia against hospital deaths\n",
    "#it looks fairly inconclusive even taking into account the fact that there are not many dementia 1 labels\n",
    "# decide to remove the dementia label as an explanatory variable\n",
    "#it's also worth noting that the coefficient was negative which doesn't seem to make sense given it's a comorbidity\n",
    "\n",
    "pd.crosstab(X_correlation['dementia'],X_correlation['hospdead'],normalize='index').plot.bar(stacked=True)\n",
    "#ax=sns.countplot(X_correlation, x=\"dementia\", hue=\"hospdead\", stat=\"percent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae905b-2874-4c59-8da3-290a0139aa64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing dementia from dataframe\n",
    "X4 = X4.drop('dementia', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4044c146-fade-44aa-bc49-e729fa2be508",
   "metadata": {},
   "source": [
    "#### 4.7 Data Exploration: Conclusions from Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa72bc5-2aa8-4752-882f-fe30661c104f",
   "metadata": {},
   "source": [
    "The main conclusions from the exploratory analysis conducted:\n",
    "* Removed hday (day admitted to trial) as found to have correlation with both totcst (0.46) and charges (0.47) and found to be spurious as it’s noted the longer someone is in hospital presumably the more likely they are to subsequently die.\n",
    "* Decided to drop charges from the features under consideration as it had a high correlation (0.77) with totcst yet was not found to be as good a separator of hospital death and survival .\n",
    "* Considering there is a strong correlation between avtisst and aps (0.59)  with avtisst less clear a separator of hospital survival and deaths; worth experimenting with removing and retaining avtisst.\n",
    "* In cases where avtisst is retained consider experimenting with excluding totcst given significant correlation with avtisst (0.46) (avtisst is the more effective separator of the two).\n",
    "* Removed dementia from features as it was deemed the relationship was spurious\n",
    "* Remapped DNR encoding to reflect DNR after admission having a clearer impact than DNR before admission  (i.e from 1 to 2 and vice versa)\n",
    "* All remaining features not already mentioned under consideration found to have a credible relationship with hospital deaths remain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2793c2a1-f5d3-41ea-85a2-8a951d7913b1",
   "metadata": {},
   "source": [
    "## 5 Models 2 (X4 data), 3 (X5 data), 4 (X6 data) Logistic Regression on curated data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d8154f-5f36-4746-ae43-fd797fe056ee",
   "metadata": {},
   "source": [
    "Having made some further adjustments to the data under consideration mainly dropping certain variables and remapping some others I determined to train new logistic models on the curated data X4, and also to include permutations of including and excluding each of avtisst and totcst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07107b-d3d6-4da3-99e0-9c1d6b587620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#create new dataframe X5 with totcst removed (but with avtisst included)\n",
    "X5=X4\n",
    "X5=X5.drop('totcst', axis=1)\n",
    "X5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c139db9a-1a46-4d82-870d-3ff4491cd3ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create new dataframe X6 with avtisst removed (but with totscst included)\n",
    "X6=X4\n",
    "X6=X6.drop('avtisst', axis=1)\n",
    "X6.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a86bcaf-9b8a-4cd3-87e8-787bf7680c7c",
   "metadata": {},
   "source": [
    "### 5.1: Model 2 Logistic Regression (X4 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b29e8e-e776-408b-8783-5f475e78e4da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train model 2 on curated X4 data\n",
    "# Splitting data into training and testing sets\n",
    "X_train2, X_test2, Y_train, Y_test = train_test_split(X4,Y, test_size=0.2, random_state=3)\n",
    "\n",
    "\n",
    "# Create and fit the linear regression model\n",
    "model_log2 = LogisticRegression()\n",
    "logistic_fit2=model_log2.fit(X_train2, Y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "Y_pred_log2 = np.round(model_log2.predict(X_test2)) # rounded\n",
    "\n",
    "\n",
    "Model_2 = MNLogisticModel(X_train2, Y_train)\n",
    "\n",
    "print(Model_2[0].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b476b-99ce-497f-92b7-655a3aeda718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test,  Y_pred_log2)\n",
    "print(f\"Accuracy of the model: {accuracy:.2f}\")\n",
    "\n",
    "precision_micro = precision_score(Y_test, Y_pred_log2, average='micro')\n",
    "recall_micro = recall_score(Y_test,Y_pred_log2, average='micro')\n",
    "\n",
    "r2 = r2_score(Y_test, Y_pred_log2)\n",
    "mse = mean_squared_error(Y_test, Y_pred_log2)\n",
    "\n",
    "\n",
    "print(f'Micro Precision: {precision_micro}')\n",
    "print(f'Micro Recall: {recall_micro}')\n",
    "print(f'r2: {r2}')\n",
    "print(f'mse: {mse}')\n",
    "print(f'bic: {Model_2[0].bic}')\n",
    "\n",
    "# Display confusion matrix\n",
    "confusion_matrix = pd.crosstab(Y_test['hospdead'], Y_pred_log2.flatten(), rownames=['Actual'], colnames=['Predicted'])\n",
    "sn.heatmap(confusion_matrix, annot=True)\n",
    "plt.show()\n",
    "\n",
    "#we observe no improvement in accuracy but a marginal improvement in precision and recall from 82% to 83%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fdb2ad-bdc9-42f3-8243-1b7bcce2cd5b",
   "metadata": {},
   "source": [
    "### 5.2: Model 3 Logistic Regression (X5 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f3dd0-fbcf-46a2-9b12-0e26b5ee811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model 3 on curated X5 data\n",
    "#this dataset has had totcst removed otherwise it is the same as X4\n",
    "\n",
    "\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train3, X_test3, Y_train, Y_test = train_test_split(X5,Y, test_size=0.2, random_state=3)\n",
    "\n",
    "\n",
    "# Create and fit the linear regression model\n",
    "model_log3 = LogisticRegression()\n",
    "logistic_fit3=model_log3.fit(X_train3, Y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "Y_pred_log3 = np.round(model_log3.predict(X_test3)) # rounded\n",
    "\n",
    "\n",
    "Model_3 = MNLogisticModel(X_train3, Y_train)\n",
    "\n",
    "print(Model_3[0].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b8b46-3ba3-4296-8aba-d6b71bc7d9a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test,  Y_pred_log3)\n",
    "print(f\"Accuracy of the model: {accuracy:.2f}\")\n",
    "\n",
    "precision_micro = precision_score(Y_test, Y_pred_log3, average='micro')\n",
    "recall_micro = recall_score(Y_test,Y_pred_log3, average='micro')\n",
    "\n",
    "\n",
    "r2 = r2_score(Y_test, Y_pred_log3)\n",
    "mse = mean_squared_error(Y_test, Y_pred_log3)\n",
    "\n",
    "\n",
    "print(f'Micro Precision: {precision_micro}')\n",
    "print(f'Micro Recall: {recall_micro}')\n",
    "print(f'r2: {r2}')\n",
    "print(f'mse: {mse}')\n",
    "print(f'bic: {Model_3[0].bic}')\n",
    "\n",
    "# Display confusion matrix\n",
    "confusion_matrix = pd.crosstab(Y_test['hospdead'], Y_pred_log3.flatten(), rownames=['Actual'], colnames=['Predicted'])\n",
    "sn.heatmap(confusion_matrix, annot=True)\n",
    "plt.show()\n",
    "\n",
    "#we observe a very substantial improvement in accuracy from 83% in model 1 to 88%\n",
    "#in addition we observe a substantial improvement in precision and recall from 82% to 88%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac464cd-5e00-4ab0-8ce1-f13eb183cfd2",
   "metadata": {},
   "source": [
    "### 5.3: Model 4 Logistic Regression (X6 data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b4bb4-d642-4a84-9842-fb86d5298045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train model 4 on curated X6 data\n",
    "#this dataset has had avtisst removed otherwise it is the same as X4\n",
    "\n",
    "\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train4, X_test4, Y_train, Y_test = train_test_split(X6,Y, test_size=0.2, random_state=3)\n",
    "\n",
    "\n",
    "# Create and fit the linear regression model\n",
    "model_log4 = LogisticRegression()\n",
    "logistic_fit4=model_log4.fit(X_train4, Y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "Y_pred_log4 = np.round(model_log4.predict(X_test4)) # rounded\n",
    "\n",
    "\n",
    "Model_4 = MNLogisticModel(X_train4, Y_train)\n",
    "\n",
    "print(Model_4[0].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e35ca7-8b15-4137-a742-e19fa882c55d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test,  Y_pred_log4)\n",
    "print(f\"Accuracy of the model: {accuracy:.2f}\")\n",
    "\n",
    "precision_micro = precision_score(Y_test, Y_pred_log4, average='micro')\n",
    "recall_micro = recall_score(Y_test,Y_pred_log4, average='micro')\n",
    "\n",
    "\n",
    "\n",
    "r2 = r2_score(Y_test, Y_pred_log4)\n",
    "mse = mean_squared_error(Y_test, Y_pred_log4)\n",
    "\n",
    "\n",
    "print(f'Micro Precision: {precision_micro}')\n",
    "print(f'Micro Recall: {recall_micro}')\n",
    "print(f'r2: {r2}')\n",
    "print(f'mse: {mse}')\n",
    "print(f'bic: {Model_4[0].bic}')\n",
    "\n",
    "# Display confusion matrix\n",
    "confusion_matrix = pd.crosstab(Y_test['hospdead'], Y_pred_log4.flatten(), rownames=['Actual'], colnames=['Predicted'])\n",
    "sn.heatmap(confusion_matrix, annot=True)\n",
    "plt.show()\n",
    "\n",
    "#we see that removing avtiss but retaining totcst leads to a substantial reduction in performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c7c1c-a8e1-455b-a962-dc5577d2bc3d",
   "metadata": {},
   "source": [
    "### 5.4: Summary of Models 1-4 \n",
    "\n",
    "Before going onto consideration of the neural network and decision tree models I recap the model performance of the logistic regression.\n",
    "\n",
    "It is found that Model 3 has by far the best performance accross most of the metrics considered. With an accuracy of 88% it improved substantially on Model 1 which had included more features. The main reason for the improvement I believe is the removal of redundant coefficients as well as the remapping of some others.\n",
    "\n",
    "We see however that Model 3 has a higher BIC score than Models 1 and 2.\n",
    "\n",
    "| Model | Accuracy | Micro Precision | Micro Recall| BIC| \n",
    "|:--------:|:--------:|:--------:|:--------:|:--------:|\n",
    "| Model 1: Logistic Regression (All Features)  |  0.83  |  0.83   |  0.83   |4217 |\n",
    "| Model 2: Logistic Regression  Significant Features Only  |  0.83  | 0.83  | 0.83  | 4208|\n",
    "|  **Model 3: Logistic Regression ex totcst**  |  0.88  |  0.88  |  0.88   | 4223 |\n",
    "|  Model 4: Logistic Regression ex avtisst  |  0.81   |  0.81  |   0.81   |   5047 |   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a619f3-547f-45dc-8589-6e9e2880297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Model_1 = MNLogisticModel (X_train, Y_train)\n",
    "# Create and fit the linear regression model\n",
    "model_log = LogisticRegression()\n",
    "logistic_fit=model_log.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "Y_pred_log = np.round(model_log.predict(X_test)) # rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b8329-252d-4407-bff2-957b83e0214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def fit_model(predictors):\n",
    "    X_sm = sm.add_constant(X_df[list(predictors)]) #include the intercept\n",
    "    model = sm.OLS(y, X_sm).fit()\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae06c4-00b0-44b3-86fc-593d2237b5c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# used for constructing all possible subsets\n",
    "import itertools\n",
    "\n",
    "def best_subset_selection(X, Y):\n",
    "    # Initialize variables\n",
    "    best_models = []\n",
    "    num_predictors = X.shape[1]\n",
    "    \n",
    "    # Loop over k = 1 to k = p predictors\n",
    "    for k in range(1, num_predictors + 1):\n",
    "        current_models = []\n",
    "        \n",
    "        # Loop over all possible combinations: choose k predictors from num_predictors\n",
    "        for combo in itertools.combinations(X.columns, k):\n",
    "            X_subset = X[list(combo)]\n",
    "            \n",
    "            # Add a constant term (intercept) to predictors\n",
    "            X_subset_const = sm.add_constant(X_subset)\n",
    "            \n",
    "            # Fit the model\n",
    "            model = sm.OLS(Y, X_subset_const).fit()\n",
    "            current_models.append((combo, model))\n",
    "            \n",
    "        # Find best model based on R-squared\n",
    "        best_current_model = max(current_models, key=lambda tup: tup[1].rsquared)\n",
    "        best_models.append([best_current_model[0], best_current_model[1].rsquared]) \n",
    "        \n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa383d49-c70f-441d-9a60-745fd7e7b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "def fit_model(predictors):\n",
    "    X_sm = sm.add_constant(X_df[list(predictors)]) #include the intercept\n",
    "    model = sm.OLS(y, X_sm).fit()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6a4890-9616-4755-811f-d8d93355d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def best_subset_selection(X, Y):\n",
    "    # Initialize variables\n",
    "    best_models = []\n",
    "    num_predictors = X.shape[1]\n",
    "    \n",
    "    # Loop over k = 1 to k = p predictors\n",
    "    for k in range(1, num_predictors + 1):\n",
    "        current_models = []\n",
    "        \n",
    "        # Loop over all possible combinations: choose k predictors from num_predictors\n",
    "        for combo in itertools.combinations(X.columns, k):\n",
    "            X_subset = X[list(combo)]\n",
    "            \n",
    "            # Add a constant term (intercept) to predictors\n",
    "            X_subset_const = sm.add_constant(X_subset)\n",
    "            \n",
    "            # Fit the model\n",
    "            model = sm.OLS(Y, X_subset_const).fit()\n",
    "            current_models.append((combo, model))\n",
    "            \n",
    "        # Find best model based on R-squared\n",
    "        best_current_model = max(current_models, key=lambda tup: tup[1].rsquared)\n",
    "        best_models.append([best_current_model[0], best_current_model[1].rsquared]) \n",
    "        \n",
    "    return best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86aa62-b9ea-473e-8131-ba5825b6159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the linear regression model\n",
    "model_log = LogisticRegression()\n",
    "logistic_fit=model_log.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "Y_pred_log = np.round(model_log.predict(X_test)) # rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b518e0b6-450e-4b59-a123-5d1d77de5ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Model_1 = MNLogisticModel (X_train, Y_train)\n",
    "# Create and fit the linear regression model\n",
    "model_log = LogisticRegression()\n",
    "logistic_fit=model_log.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "Y_pred_log = np.round(model_log.predict(X_test)) # rounded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7614d1-c9ae-461d-8725-72e5e6337994",
   "metadata": {},
   "source": [
    "## 6: Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea769b6-181a-4723-a201-b3f8020e0676",
   "metadata": {},
   "source": [
    "Since the problem relates to a classification problem a softmax is used for the final layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ccee0e-7b85-402e-862a-fe8004eca9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X5.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f72883-3ea6-4740-88e3-d3bfdbc621a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y2=Y.copy()\n",
    "\n",
    "#Y2['hospdead']=Y2['hospdead'].astype(float)\n",
    "#Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e37dfd-dcb3-4820-9c8b-fa67e9e7d3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X5.dtypes\n",
    "\n",
    "X_NN = X5.copy()\n",
    "#scaling to standardise all inputs given wide range of values accross features\n",
    "scaler = StandardScaler()\n",
    "X_NN = scaler.fit_transform(X_NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9351fbc1-b110-4c07-9cdd-a0a1aaf40832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test_tensor = torch.Tensor(test.values)\n",
    "\n",
    "\n",
    "\n",
    "X5.dtypes\n",
    "X_NN = X5.copy()\n",
    "#scaling to standardise all inputs given wide range of values accross features\n",
    "scaler = StandardScaler()\n",
    "X_NN = scaler.fit_transform(X_NN)\n",
    "\n",
    "Y2=Y.copy()\n",
    "\n",
    "X_NN = torch.tensor(X_NN, dtype=torch.float32)\n",
    "y = torch.tensor(Y2.values, dtype=torch.float32)\n",
    "\n",
    "# Split the dataset into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_NN, y, test_size=0.2, random_state=23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f21a83-ac72-4bf3-b441-9b1957ad582f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd24f3-c810-4154-8ff0-b2fcea626f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(8, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 1),\n",
    "  \n",
    "    \n",
    "    nn.Sigmoid())\n",
    "    #nn.Softmax(dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaac27ab-1cc8-4f61-b38d-db4a1fb6a01a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65057f9e-b4cc-42c8-95ff-45def0b47400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#since binary classification problem using binary cross entropy\n",
    "loss_fn = nn.BCELoss() \n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8bd643-4a60-479b-baf7-32ae21a74bac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_plot(epochs, loss):\n",
    "    plt.plot(epochs, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920b5476-1d48-4b3f-8a35-345cfd59fc09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    \n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test)\n",
    "accuracy = (y_pred.round() == y_test).float().mean()\n",
    "print(f\"Accuracy {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8b5963-6f2e-418a-8e95-44882b0ff061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f50503-8b6a-4bca-9db7-15009bad1283",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_NN, y, test_size=0.2, random_state=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae2f061-30ba-44c9-88bf-4cde89b94066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#def accuracy_plot(epochs, train_accuracy_vals,test_accuracy_vals):\n",
    "def accuracy_plot( train_accuracy_vals,test_accuracy_vals):  \n",
    "    \n",
    "    #plt.plot(range(1, epochs + 1), train_accuracy_vals, label='Training Accuracy')\n",
    "    plt.plot(test_accuracy_vals, label='Validation Accuracy')\n",
    "    plt.plot( train_accuracy_vals, label='Training Accuracy')\n",
    "    \n",
    "    \n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training Accuracy vs. Epoch')\n",
    "    #plt.plot(epochs, loss_vals)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f52da1a-78fe-4b3a-a134-b92222fa807a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 500\n",
    "\n",
    "#loss_vals=  []\n",
    "test_accuracy_vals=  []\n",
    "train_accuracy_vals=  []\n",
    " \n",
    "for epoch in range(n_epochs):\n",
    "    #for the chart\n",
    "    epoch_test_accuracy= []\n",
    "    epoch_train_accuracy= []\n",
    "    \n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        Xbatch = X_train[i:i+batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        ybatch = y_train[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "\n",
    "       \n",
    "   \n",
    "       \n",
    "        \n",
    "        y_pred_train = model(Xbatch)\n",
    "        #train_accuracy = (y_pred_train.round() == ybatch).float().mean()\n",
    "        train_accuracy  = accuracy_score(y_pred_train.round().detach(),  ybatch.detach())\n",
    "        epoch_train_accuracy.append(train_accuracy.item()) \n",
    "        \n",
    "    \n",
    "        y_pred_test = model(X_test)\n",
    "        #test_accuracy = (y_pred_test.round() == y_test).float().mean()\n",
    "        test_accuracy  = accuracy_score(y_pred_test.round().detach(), y_test.detach()  )\n",
    "        epoch_test_accuracy.append(test_accuracy.item()) \n",
    "    #loss_vals.append(sum(epoch_loss)/len(epoch_loss))\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    #test_accuracy_vals.append(sum(epoch_test_accuracy)/len(epoch_test_accuracy))\n",
    "    test_accuracy_vals.append(sum(epoch_test_accuracy)/len(epoch_test_accuracy))\n",
    "    train_accuracy_vals.append(sum(epoch_train_accuracy)/len( epoch_train_accuracy))\n",
    "    \n",
    "    #accuracy = accuracy_score(Y_test,  Y_pred_log4)\n",
    "    \n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}, latest train accuracy {  train_accuracy}')\n",
    "\n",
    "    \n",
    "\n",
    "#accuracy_plot(np.linspace(1, n_epochs).astype(int),train_accuracy_vals,test_accuracy_vals)\n",
    "accuracy_plot(train_accuracy_vals,test_accuracy_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e14eea-6abd-43d7-a535-c89f07837765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7911f0e-fb73-48e7-b87b-9fac71fc97a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07980006-f46a-4c36-b8c2-52849855a964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
